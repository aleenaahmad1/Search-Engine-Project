import os
import time
import json 
import nltk
import re
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
from nltk.stem.snowball import SnowballStemmer


snow_stemmer = SnowballStemmer(language='english')


#global data:
docID = 0        
titleID = 0         #assign unique ids to word, doc and title
wordID = 0
finaldocs = {}
lexicon= {}         #dictionary for lexicon of article content
titlelex = {}       #lexicon for titles
fwdIndexContent = {}
fwdIndexTitle = {}
invertedIndexContent = {}
invertedIndexTitle = {}     
stop_words = set(stopwords.words('english')) 
stopwords_dict = Counter(stop_words)

#FUNCTIONS:

def stemming(wordlist): 
  wordsfinal = []
  for word in wordlist:
    wordsfinal.append(snow_stemmer.stem(word))
  return wordsfinal

#titlewords = ""
def tokenize(dict, name):
    tokens =[]
    #content = dict["content"] + " " + dict["title"]
    if name=="content":
        content = dict["content"]
    else:
        content = dict["title"]
    tokens.extend(word_tokenize(content)) #list of lists
    noStopWords = []
    for word in tokens:
        word = word.lower()
        if (word.isalnum() and word.casefold() not in stop_words):
            noStopWords.append(word)
    final = stemming(noStopWords)
    return final

def createlexicon(wordlist, name):
    if name=="content":
        global wordID
        for word in wordlist:
            if lexicon.get(word) is not None:
                pass
            else:
                lexicon.update({word:wordID})
                wordID+=1
    else:
        global titleID
        for word in wordlist:
            if titlelex.get(word) is not None:
                pass
            else:
                titlelex.update({word:wordID})
                wordID+=1
    

def createFwdIndex(content, name):
    global fwdIndexContent
    global fwdIndexTitle
    global docID
    innerdict = {}
    if name == "content":
        for index in range (len(content)):   #accesses each word of content 
            counter = 0
            ID = lexicon.get(content[index])
            #position = index
            if innerdict.get(ID) is None:
                counter+=1
                # hitlist = []
                # hitlist.append(position)
                # hitlist.append(counter)
                innerdict.update({ID:counter})
            else:
                counter=innerdict.get(ID)
                counter+=1
                innerdict[ID]=counter
                # innerdict.get(ID).append(position)
                # innerdict.get(ID).append(counter)         
        fwdIndexContent.update({docID-1:innerdict})
    else:
        for index in range (len(content)):   #accesses each word of content 
            counter = 0
            ID = titlelex.get(content[index])
            position = index
            if innerdict.get(ID) is None:
                counter+=1
                # hitlist = []
                # hitlist.append(position)
                # hitlist.append(counter)
                innerdict.update({ID:counter})
            else:
                counter=innerdict.get(ID)
                counter+=1
                innerdict[ID]=counter
                # innerdict.get(ID).append(position)
                # innerdict.get(ID).append(counter)         
        fwdIndexTitle.update({docID-1:innerdict})


#FWD INDEX STRUCTURE: {docID: {wordID: hits, wordID: hits}
#                      docID: {wordID: hits, wordID: hits}}

def createInvertedIndex(name):
    if name=="content":
        for keys in fwdIndexContent: #each key: docID
            inner = fwdIndexContent.get(keys)#dict of each doc, with words and their hits 
            for key2 in inner: #WORDS THAT ARE IN THE DOCUMENT
                if invertedIndexContent.get(key2) is not None:
                    invertedIndexContent[key2].append(keys)
                else:
                    list1 = [keys]
                    invertedIndexContent.update({key2:list1})
    else:
        for keys in fwdIndexTitle: #each key: docID
            inner = fwdIndexTitle.get(keys)#dict of each doc, with words + positions 
            for key2 in inner: #WORDS THAT ARE IN THE DOCUMENT
                if invertedIndexTitle.get(key2) is not None:
                    invertedIndexTitle[key2].append(keys)
                else:
                    list1 = [keys]
                    invertedIndexTitle.update({key2:list1})


filename1 = "fwdIndexContentFinal.json"     
filename2 = "lexiconFinal.json"         #names of final files to be passed to function
filename3 = "titlelexFinal.json"
filename4 = "invertedIndexContentFinal.json"
filename5 = "fwdIndexTitleFinal.json"
filename6 = "invertedIndexTitleFinal.json"
filename7 = "docIndexFinal.json"

def writeFiles(dict, filename):
    filename = finalpath + "\\" + filename
    json_object = json.dumps(dict)
    with open(filename, "w", encoding='utf-8') as finalfile:
        finalfile.write(json_object)

docindex = {}
def createDocIndex(dictionary, filenumber,docID):
        global docindex
        docinfo = []
        docinfo.append(dictionary.get("url"))
        docinfo.append(dictionary.get("title"))
        docinfo.append(filenumber)
        docindex.update({docID:docinfo})


##SPECIFYING PATHS OF DATA:

destpath = "E:\codes\python\cleanfiles" #PATH for folder where final, tokenized content of docs will be stored
sourcepath = "E:\samplef" #folder containing data files
finalpath = "E:\indexfilesfinal"



#Processing:

filenum = -1

start = time.time()
for file in os.listdir(sourcepath): #369, 911truth
    name = destpath+ "\\" +file
    file = sourcepath+"\\"+file
    myfile = open(file)
    data = json.load(myfile) #returns a list(of dictionaries?)
    filenum+=1
    for dict in data: #each dictionary represents ONE article
        #createDocIndex(dict, filenum,docID)
        #finaldocs.clear()

        # TOKENIZING CONTENT:
        #tokenizetime = time.time()
        finalWords = tokenize(dict, "content")
        #print("tokenizing time for 1 file:", time.time()-tokenizetime)
        # TOKENIZING TITLES:
        finalTitles = tokenize(dict, "title")

#         #WRITING TOKENIZED CONTENT TO FINAL FILE
        # finaldocs.update({docID:dict["title"]})
        # finaldocs.update({"content":content})
        docID+=1
#         #makeFile(name)

#         #CREATING lexiconS:
        #lextime = time.time()
        createlexicon(finalWords, "content")
        #print("lextime:", time.time()-lextime)
        createlexicon(finalTitles, "title")  
        #fwdindex = time.time()      
        createFwdIndex(finalWords,"content")
        #print("lextime:", time.time()-lextime)
        createFwdIndex(finalTitles, "title")
    myfile.close()
print("time for token+lex+fwd", time.time()-start)


#CREATING INVERTED INDEX:
invIndextime = time.time()
createInvertedIndex("content")
print("INV INDEX content TIME:", time.time()-invIndextime)
invIndextime = time.time()
createInvertedIndex("title")
print("INV INDEX content TIME:", time.time()-invIndextime)


writeFiles(fwdIndexContent, filename1)
writeFiles(lexicon, filename2)
#writeFiles(titlelex, filename3)
writeFiles(invertedIndexContent,filename4)
#writeFiles(fwdIndexTitle,filename5)
#writeFiles(invertedIndexTitle,filename6)
#writeFiles(docindex, filename7)
