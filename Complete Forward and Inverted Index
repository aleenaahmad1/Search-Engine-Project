import os
import json 
import nltk
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet


#global data:
docID = 0        
titleID = 0         #assign unique ids to word, doc and title
wordID = 0
finaldocs = {}
lexicon= {}         #dictionary for lexicon of article content
titlelex = {}       #lexicon for titles
fwdIndexContent = {}
fwdIndexTitle = {}
invertedIndexContent = {}
invertedIndexTitle = {}
docindex = {}
filenum = -1


#FUNCTIONS:

def tokenize(dict, name):
    tokens = []
    if(name=="content"):
        tokens.append(word_tokenize(dict["content"])) #list of lists
    else:
        tokens.append(word_tokenize(dict["title"])) #list of lists
    noStopWords = []
    stop_words = stopwords.words('english') 
    for lists in tokens: 
        for word in lists:
            if (word.isalnum() and word.casefold() not in stop_words):
                noStopWords.append(word)
    final = lemmatize(noStopWords)
    return final

def lemmatize(wordlist):
    lemm = WordNetLemmatizer()
    wordsfinal= []
    wordlist = nltk.pos_tag(wordlist) #part of speech tags added, list of tuples
    for word in wordlist:
        if(word[1].startswith('J')):
            pos = wordnet.ADJ
        elif(word[1].startswith('V')):
            pos = wordnet.VERB
        elif(word[1].startswith('R')):
            pos = wordnet.ADV
        else:
            pos = wordnet.NOUN
        wordsfinal.append(lemm.lemmatize(word[0], pos))
    return wordsfinal

def createlexicon(wordlist):
    global wordID
    for word in wordlist:
        if(word in lexicon):
            pass
        else:
            lexicon.update({word:wordID})
            wordID+=1

def createTitleLex(wordlist):
    global titleID
    for word in wordlist:
        if(word in titlelex):
            pass
        else:
            titlelex.update({word:titleID})
            titleID+=1

def createFwdIndex(content,dictionary, name):
    global fwdIndexContent
    temp = ""
    if name == "content":
        temp = lexicon
    else:
        temp = titlelex
    innerdict = {}
    for index in range (len(content)):   #accesses each word of content 
        counter = 0
        ID = temp.get(content[index])
        position = index
        if innerdict.get(ID) is None:
            counter+=1
            hitlist = []
            hitlist.append(position)
            hitlist.append(counter)
            innerdict.update({ID:hitlist})
        else:
            counter=innerdict[ID].pop()
            counter+=1
            innerdict.get(ID).append(position)
            innerdict.get(ID).append(counter)         
    dictionary.update({docID-1:innerdict})


def createInvertedIndex(word,dictionary):
    doclist = []
    wordID = word
    temp = ""
    if dictionary == invertedIndexContent:
        temp = fwdIndexContent
    else:
        temp = fwdIndexTitle
    for keys in temp:
        if wordID in temp.get(keys):
            doclist.append(keys)
    dictionary.update({wordID:doclist})

def createDocIndex(dictionary, filenumber,docID):
        global docindex
        docinfo = []
        docinfo.append(dictionary.get("url"))
        docinfo.append(dictionary.get("title"))
        docinfo.append(filenumber)
        docindex.update({docID:docinfo})

filename1 = "fwdIndexContentFinal.json"     
filename2 = "lexiconFinal.json"         #names of final files to be passed to function
filename3 = "titlelexFinal.json"
filename4 = "invertedIndexContentFinal.json"
filename5 = "fwdIndexTitleFinal.json"
filename6 = "invertedIndexTitleFinal.json"
filename7 = "docIndexFinal.json"

def writeFiles(dict, filename):
    filename = finalpath + "\\" + filename
    json_object = json.dumps(dict)
    with open(filename, "w", encoding='utf-8') as finalfile:
        finalfile.write(json_object)

##SPECIFYING PATHS OF DATA:

destpath = "E:\codes\python\cleanfiles" #PATH for folder where final, tokenized content of docs will be stored
sourcepath = "E:\sample" #folder containing data files
finalpath = "E:\indexfiles"

#Processing:

for file in os.listdir(sourcepath): #369, 911truth
    name = destpath+ "\\" +file
    file = sourcepath+"\\"+file
    myfile = open(file)
    data = json.load(myfile) #returns a list(of dictionaries?)
    filenum+=1
    for dict in data: #i=each article (each dictionary) in data
        createDocIndex(dict, filenum,docID)
        finaldocs.clear()

        #TOKENIZING CONTENT:
        finalWords = tokenize(dict, "content")

        #TOKENIZING TITLES:
        finalTitles = tokenize(dict, "title")

        #WRITING TOKENIZED CONTENT TO FINAL FILE
        finaldocs.update({docID:dict["title"]})
        finaldocs.update({"content":content})
        docID+=1
        #makeFile(name)

        # #CREATING lexiconS:
        createlexicon(finalWords)
        createTitleLex(finalTitles)        
        createFwdIndex(finalWords,fwdIndexContent,"content")
        createFwdIndex(finalTitles,fwdIndexTitle, "title")

    #CREATING INVERTED INDEX:
    for lexi in lexicon:
        createInvertedIndex(lexicon[lexi],invertedIndexContent)
    for lexi in titlelex:
        createInvertedIndex(titlelex[lexi],invertedIndexTitle)




writeFiles(fwdIndexContent, filename1)
writeFiles(lexicon, filename2)
writeFiles(titlelex, filename3)
writeFiles(invertedIndexContent,filename4)
writeFiles(fwdIndexTitle,filename5)
writeFiles(invertedIndexTitle,filename6)
writeFiles(docindex, filename7)
