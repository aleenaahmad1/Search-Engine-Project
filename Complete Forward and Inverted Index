import os
import time
import json 
import nltk
import re
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter


#global data:
docID = 0        
titleID = 0         #assign unique ids to word, doc and title
wordID = 0
finaldocs = {}
lexicon= {}         #dictionary for lexicon of article content
titlelex = {}       #lexicon for titles
fwdIndexContent = {}
fwdIndexTitle = {}
invertedIndexContent = {}
invertedIndexTitle = {}     
stop_words = set(stopwords.words('english')) 
stopwords_dict = Counter(stop_words)

#FUNCTIONS:

def lemmatize(wordlist):
    lemm = WordNetLemmatizer()
    wordsfinal= []
    wordlist = nltk.pos_tag(wordlist) #part of speech tags added, list of tuples
    for word in wordlist:
        if(word[1].startswith('J')):
            pos = wordnet.ADJ
        elif(word[1].startswith('V')):
            pos = wordnet.VERB
        elif(word[1].startswith('R')):
            pos = wordnet.ADV
        else:
            pos = wordnet.NOUN
        wordsfinal.append(lemm.lemmatize(word[0], pos))
    return wordsfinal


def tokenize(dict):
    tokens =[]
        #dict[content] is a string
        # content = re.sub("[^\w\s]", "", dict["content"])
        # fincontent = [word.lower() for word in content.split() if word.casefold() not in stopwords_dict ]
        # tokens.extend(fincontent)
    content = dict["content"] + " " + dict["title"]
    tokens.extend(word_tokenize(content)) #list of lists
    noStopWords = []
    for word in tokens:
        #word = word.lower()
        # if (not(word.isalnum()) or word.casefold() in stop_words):
        #     tokens.remove(word)
        if (word.isalnum() and word.casefold() not in stop_words):
            noStopWords.append(word)
    final = lemmatize(noStopWords)
    return final

def createlexicon(wordlist):
    global wordID
    for word in wordlist:
        if lexicon.get(word) is not None:
            pass
        else:
            lexicon.update({word:wordID})
            wordID+=1



# def createFwdIndex():
#     global fwdIndexContent
#     innerdict = {}
#     position = 0
#     for keys in lexicon:
#         ID = lexicon[keys]
#         counter = 0
#         position+=1
#         if innerdict.get(ID) is None:
#             counter+=1
#             hitlist = []
#             hitlist.append(position-1)
#             hitlist.append(counter)
#             innerdict.update({ID:hitlist})
#         else:
#             counter=innerdict[ID].pop()
#             counter+=1
#             innerdict.get(ID).append(position-1)
#             innerdict.get(ID).append(counter)         
#     fwdIndexContent.update({docID:innerdict})




def createFwdIndex(content,dictionary):
    global fwdIndexContent
    innerdict = {}
    for index in range (len(content)):   #accesses each word of content 
        counter = 0
        ID = dictionary.get(content[index])
        position = index
        if innerdict.get(ID) is None:
            counter+=1
            hitlist = []
            hitlist.append(position)
            hitlist.append(counter)
            innerdict.update({ID:hitlist})
        else:
            counter=innerdict[ID].pop()
            counter+=1
            innerdict.get(ID).append(position)
            innerdict.get(ID).append(counter)         
    fwdIndexContent.update({docID-1:innerdict})


def createInvertedIndex():
    for keys in fwdIndexContent: #each key: docID
        inner = fwdIndexContent.get(keys)#dict of each doc, with words + positions 
        for key2 in inner: #WORDS THAT ARE IN THE DOCUMENT
            if invertedIndexContent.get(key2) is not None:
                invertedIndexContent[key2].append(keys)
            else:
                list1 = [keys]
                invertedIndexContent.update({key2:list1})

filename1 = "fwdIndexContentFinal.json"     
filename2 = "lexiconFinal.json"         #names of final files to be passed to function
#filename3 = "titlelexFinal.json"
filename4 = "invertedIndexContentFinal.json"
#filename5 = "fwdIndexTitleFinal.json"
#filename6 = "invertedIndexTitleFinal.json"
filename7 = "docIndexFinal.json"

def writeFiles(dict, filename):
    filename = samplepath + "\\" + filename
    json_object = json.dumps(dict)
    with open(filename, "w", encoding='utf-8') as finalfile:
        finalfile.write(json_object)

docindex = {}
def createDocIndex(dictionary, filenumber,docID):
        global docindex
        docinfo = []
        docinfo.append(dictionary.get("url"))
        docinfo.append(dictionary.get("title"))
        docinfo.append(filenumber)
        docindex.update({docID:docinfo})


##SPECIFYING PATHS OF DATA:

destpath = "E:\codes\python\cleanfiles" #PATH for folder where final, tokenized content of docs will be stored
sourcepath = "E:\sample1" #folder containing data files
finalpath = "E:\indexfiles"
samplepath = "E:\check"


#Processing:

filenum = -1

start = time.time()
for file in os.listdir(sourcepath): #369, 911truth
    name = destpath+ "\\" +file
    file = sourcepath+"\\"+file
    myfile = open(file)
    data = json.load(myfile) #returns a list(of dictionaries?)
    filenum+=1
    for dict in data: #each dictionary represents ONE article
        # createDocIndex(dict, filenum,docID)
        finaldocs.clear()

        # TOKENIZING CONTENT:
        #tokenizetime = time.time()
        finalWords = tokenize(dict)
        #print("tokenizing time for 1 file:", time.time()-tokenizetime)
        # TOKENIZING TITLES:
        #finalTitles = tokenize(dict, "title")

#         #WRITING TOKENIZED CONTENT TO FINAL FILE
        # finaldocs.update({docID:dict["title"]})
        # finaldocs.update({"content":content})
        docID+=1
#         #makeFile(name)

#         #CREATING lexiconS:
        #lextime = time.time()
        createlexicon(finalWords)
        #print("lextime:", time.time()-lextime)
        #createTitleLex(finalTitles)  
        #fwdindex = time.time()      
        # createFwdIndex(finalWords, lexicon)
        #print("lextime:", time.time()-lextime)
       # createFwdIndex(finalTitles,fwdIndexTitle, "title")
    myfile.close()
print("time for token+lex+fwd", time.time()-start)


#CREATING INVERTED INDEX:
invIndextime = time.time()
createInvertedIndex()
print("INV INDEX TIME:", time.time()-invIndextime)




#print(invertedIndexContent)
# writeFiles(fwdIndexContent, filename1)
# writeFiles(lexicon, filename2)
#writeFiles(titlelex, filename3)/
# writeFiles(invertedIndexContent,filename4)
#writeFiles(fwdIndexTitle,filename5)
#writeFiles(invertedIndexTitle,filename6)
# writeFiles(docindex, filename7)
